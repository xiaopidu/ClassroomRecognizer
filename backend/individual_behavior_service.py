#!/usr/bin/env python3
"""
ä¸ªäººè¡Œä¸ºåˆ†ææœåŠ¡
ç»“åˆInsightFaceäººè„¸è¯†åˆ«å’ŒYOLOv8å§¿æ€æ£€æµ‹ï¼Œåˆ†ææŒ‡å®šå­¦ç”Ÿçš„è¡Œä¸º
"""

import cv2
import numpy as np
from ultralytics import YOLO
from insightface.app import FaceAnalysis
import logging
from typing import Dict, List, Any, Optional, Tuple
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IndividualBehaviorAnalyzer:
    def __init__(self, face_app, behavior_params=None):
        """
        åˆå§‹åŒ–ä¸ªäººè¡Œä¸ºåˆ†æå™¨
        
        Args:
            face_app: InsightFaceäººè„¸åˆ†æåº”ç”¨å®ä¾‹
            behavior_params: è¡Œä¸ºåˆ†æå‚æ•°
        """
        logger.info("æ­£åœ¨åˆå§‹åŒ–ä¸ªäººè¡Œä¸ºåˆ†æå™¨...")
        
        # äººè„¸è¯†åˆ«
        self.face_app = face_app
        
        # åŠ è½½å§¿æ€æ£€æµ‹æ¨¡å‹
        self.pose_model = YOLO('yolov8n-pose.pt')
        logger.info("âœ“ å§¿æ€æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åŠ è½½ç‰©ä½“æ£€æµ‹æ¨¡å‹
        self.object_model = YOLO('yolov8n.pt')
        logger.info("âœ“ ç‰©ä½“æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # COCOæ•°æ®é›†çš„ç±»åˆ«æ ‡ç­¾
        self.coco_labels = [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',
            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',
            'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',
            'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
        ]
        
        # è¡Œä¸ºé¢œè‰²æ˜ å°„
        self.behavior_colors = {
            "looking_up": (0, 255, 0),      # ç»¿è‰² - æŠ¬å¤´
            "looking_down": (0, 0, 255),    # çº¢è‰² - ä½å¤´
            "neutral": (255, 255, 0),       # é»„è‰² - ä¸­æ€§
            "writing": (255, 0, 0),         # è“è‰² - å†™å­—
            "using_phone": (0, 255, 255),   # é’è‰² - ç©æ‰‹æœº
            "resting": (255, 0, 255),       # ç´«è‰² - ä¼‘æ¯
            "unknown": (128, 128, 128)      # ç°è‰² - æœªçŸ¥
        }
        
        # è¡Œä¸ºåˆ†æå‚æ•°
        self.behavior_params = {
            "head_up_threshold": 2,        # æ­£å¸¸åå§¿ç®—æŠ¬å¤´
            "head_down_threshold": 8,      # æ˜æ˜¾ä½å¤´æ‰ç®—
            "writing_threshold": 30,       # æ›´æ•æ„Ÿ
            "phone_threshold": -10,        # æ›´æ•æ„Ÿ
            "object_min_confidence": 0.5
        }
        
        if behavior_params:
            self.behavior_params.update(behavior_params)
        
        logger.info("ä¸ªäººè¡Œä¸ºåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_individual_video(
        self, 
        frames: List[np.ndarray], 
        target_student_name: str,
        student_registry: List[Dict]
    ) -> Dict[str, Any]:
        """
        åˆ†ææŒ‡å®šå­¦ç”Ÿåœ¨è§†é¢‘ä¸­çš„è¡Œä¸º
        
        Args:
            frames: è§†é¢‘å¸§åˆ—è¡¨
            target_student_name: ç›®æ ‡å­¦ç”Ÿå§“å
            student_registry: å­¦ç”Ÿæ³¨å†Œä¿¡æ¯åˆ—è¡¨
            
        Returns:
            ä¸ªäººè¡Œä¸ºåˆ†æç»“æœ
        """
        start_time = time.time()
        
        # è·å–ç›®æ ‡å­¦ç”Ÿçš„ç‰¹å¾
        target_descriptors = self._get_student_descriptors(target_student_name, student_registry)
        if not target_descriptors:
            return {
                "error": f"æœªæ‰¾åˆ°å­¦ç”Ÿ {target_student_name} çš„æ³¨å†Œä¿¡æ¯",
                "student_name": target_student_name,
                "frames_analyzed": 0
            }
        
        logger.info(f"å¼€å§‹åˆ†æå­¦ç”Ÿ {target_student_name} çš„è¡Œä¸ºï¼Œå…± {len(frames)} å¸§")
        
        # å­˜å‚¨æ¯å¸§çš„åˆ†æç»“æœ
        frame_results = []
        frames_with_student = 0
        frames_without_student = 0
        
        # åˆ†ææ¯ä¸€å¸§
        for i, frame in enumerate(frames):
            try:
                # æ¯å¸§éƒ½æ‰“å°è¿›åº¦
                progress = (i + 1) / len(frames) * 100
                logger.info(f"[ä¸ªäººåˆ†æ] è¿›åº¦: {progress:.1f}% ({i + 1}/{len(frames)})")
                
                result = self._analyze_frame_for_student(
                    frame, 
                    i,
                    target_student_name,
                    target_descriptors
                )
                
                if result["student_found"]:
                    frames_with_student += 1
                    frame_results.append(result)
                else:
                    frames_without_student += 1
                    
            except Exception as e:
                logger.error(f"å¸§ {i} å¤„ç†å¤±è´¥: {e}")
                frames_without_student += 1
                continue
        
        # æ±‡æ€»åˆ†æç»“æœ
        summary = self._summarize_individual_analysis(frame_results, target_student_name)
        
        processing_time = time.time() - start_time
        
        logger.info(f"åˆ†æå®Œæˆ: æ‰¾åˆ°å­¦ç”Ÿçš„å¸§æ•° {frames_with_student}/{len(frames)}")
        
        return {
            "student_name": target_student_name,
            "timestamp": time.time(),
            "processing_time": processing_time,
            "total_frames": len(frames),
            "frames_with_student": frames_with_student,
            "frames_without_student": frames_without_student,
            "frame_results": frame_results,
            "summary": summary
        }
    
    def _get_student_descriptors(
        self, 
        student_name: str, 
        student_registry: List[Dict]
    ) -> Optional[List[np.ndarray]]:
        """è·å–å­¦ç”Ÿçš„äººè„¸ç‰¹å¾å‘é‡"""
        for student in student_registry:
            if student.get("name") == student_name:
                descriptors = student.get("descriptors", [])
                if descriptors:
                    # å°†åˆ—è¡¨è½¬æ¢ä¸ºnumpyæ•°ç»„
                    return [np.array(desc) for desc in descriptors]
        return None
    
    def _analyze_frame_for_student(
        self,
        frame: np.ndarray,
        frame_index: int,
        target_student_name: str,
        target_descriptors: List[np.ndarray]
    ) -> Dict[str, Any]:
        """
        åœ¨å•å¸§ä¸­åˆ†æç›®æ ‡å­¦ç”Ÿçš„è¡Œä¸º
        """
        # 1. äººè„¸è¯†åˆ« - æ‰¾åˆ°ç›®æ ‡å­¦ç”Ÿ
        faces = self.face_app.get(frame)
        
        target_face = None
        max_similarity = -1
        
        for face in faces:
            # è®¡ç®—ä¸ç›®æ ‡å­¦ç”Ÿçš„ç›¸ä¼¼åº¦
            similarity = self._calculate_similarity(face.embedding, target_descriptors)
            
            if similarity > 0.5 and similarity > max_similarity:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                max_similarity = similarity
                target_face = face
        
        if not target_face:
            return {
                "frame_index": frame_index,
                "student_found": False,
                "timestamp": frame_index * 30  # æ¯30ç§’ä¸€å¸§
            }
        
        # 2. è·å–äººè„¸è¾¹ç•Œæ¡†
        face_bbox = target_face.bbox.astype(int)
        
        # 3. å§¿æ€æ£€æµ‹
        pose_results = self.pose_model(frame, verbose=False)
        
        # 4. åŒ¹é…å§¿æ€åˆ°ç›®æ ‡å­¦ç”Ÿ
        target_pose, pose_bbox = self._match_pose_to_bbox(pose_results, face_bbox)
        
        if target_pose is None:
            return {
                "frame_index": frame_index,
                "student_found": True,
                "student_name": target_student_name,
                "face_similarity": float(max_similarity),
                "pose_found": False,
                "timestamp": frame_index * 30
            }
        
        # 5. åˆ†æå§¿æ€è¡Œä¸º
        behavior = self._analyze_single_person_pose(target_pose)
        behavior["bbox"] = pose_bbox
        behavior["face_similarity"] = float(max_similarity)
        
        # 6. ç‰©ä½“æ£€æµ‹
        object_results = self.object_model(frame, verbose=False)
        desktop_objects = self._analyze_desktop_objects(object_results)
        behavior["desktop_objects"] = desktop_objects
        
        # 7. ç»˜åˆ¶æ ‡æ³¨ï¼ˆä»…ä¿å­˜éƒ¨åˆ†å¸§ï¼‰
        annotated_image = None
        # æ¯10å¸§ä¿å­˜ä¸€æ¬¡å›¾ç‰‡ï¼Œæˆ–è€…ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§
        if frame_index % 10 == 0 or frame_index == 0:
            annotated_frame = self._draw_individual_annotations(
                frame.copy(), 
                behavior, 
                target_student_name,
                face_bbox
            )
            
            # 8. è½¬æ¢ä¸ºBase64
            from behavior_service import ClassroomBehaviorAnalyzer
            temp_analyzer = ClassroomBehaviorAnalyzer()
            annotated_image = temp_analyzer._frame_to_base64(annotated_frame)
        
        return {
            "frame_index": frame_index,
            "timestamp": frame_index * 30,
            "student_found": True,
            "pose_found": True,
            "student_name": target_student_name,
            "behavior": behavior,
            "annotated_image": annotated_image
        }
    
    def _calculate_similarity(
        self, 
        embedding: np.ndarray, 
        target_descriptors: List[np.ndarray]
    ) -> float:
        """è®¡ç®—äººè„¸ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        max_sim = -1
        for desc in target_descriptors:
            # ä½™å¼¦ç›¸ä¼¼åº¦
            sim = np.dot(embedding, desc) / (np.linalg.norm(embedding) * np.linalg.norm(desc))
            max_sim = max(max_sim, sim)
        return float(max_sim)
    
    def _match_pose_to_bbox(
        self, 
        pose_results, 
        face_bbox: np.ndarray
    ) -> Tuple[Optional[Any], Optional[Dict]]:
        """
        æ ¹æ®äººè„¸bboxåŒ¹é…å¯¹åº”çš„å§¿æ€æ£€æµ‹ç»“æœ
        """
        best_match = None
        best_iou = 0
        best_bbox = None
        
        for result in pose_results:
            if result.boxes is not None and result.keypoints is not None:
                for i, box in enumerate(result.boxes):
                    pose_bbox = box.xyxy.cpu().numpy()[0].astype(int)
                    
                    # è®¡ç®—IoUï¼ˆé‡å åº¦ï¼‰
                    iou = self._calculate_iou(face_bbox, pose_bbox)
                    
                    if iou > best_iou:
                        best_iou = iou
                        best_match = result.keypoints.data[i]
                        best_bbox = {
                            "x1": int(pose_bbox[0]),
                            "y1": int(pose_bbox[1]),
                            "x2": int(pose_bbox[2]),
                            "y2": int(pose_bbox[3])
                        }
        
        if best_iou > 0.3:  # IoUé˜ˆå€¼
            return best_match, best_bbox
        
        return None, None
    
    def _calculate_iou(self, bbox1: np.ndarray, bbox2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoUï¼ˆäº¤å¹¶æ¯”ï¼‰"""
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[2], bbox2[2])
        y2 = min(bbox1[3], bbox2[3])
        
        intersection = max(0, x2 - x1) * max(0, y2 - y1)
        
        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
        
        union = area1 + area2 - intersection
        
        if union == 0:
            return 0
        
        return intersection / union
    
    def _analyze_single_person_pose(self, keypoints) -> Dict:
        """åˆ†æå•ä¸ªäººçš„å§¿æ€ï¼ˆå¤ç”¨behavior_serviceçš„é€»è¾‘ï¼‰"""
        from behavior_service import ClassroomBehaviorAnalyzer
        temp_analyzer = ClassroomBehaviorAnalyzer()
        return temp_analyzer._analyze_single_person_pose(keypoints)
    
    def _analyze_desktop_objects(self, object_results) -> List[Dict]:
        """åˆ†ææ¡Œé¢ç‰©å“ï¼ˆå¤ç”¨behavior_serviceçš„é€»è¾‘ï¼‰"""
        from behavior_service import ClassroomBehaviorAnalyzer
        temp_analyzer = ClassroomBehaviorAnalyzer()
        return temp_analyzer._analyze_desktop_objects(object_results)
    
    def _draw_individual_annotations(
        self,
        frame: np.ndarray,
        behavior: Dict,
        student_name: str,
        face_bbox: np.ndarray
    ) -> np.ndarray:
        """ç»˜åˆ¶ä¸ªäººè¡Œä¸ºæ ‡æ³¨"""
        from PIL import Image, ImageDraw, ImageFont
        
        # ä¸­è‹±æ–‡æ˜ å°„
        behavior_labels = {
            'looking_up': 'æŠ¬å¤´',
            'looking_down': 'ä½å¤´',
            'neutral': 'ä¸­æ€§',
            'writing': 'è®°ç¬”è®°',
            'using_phone': 'ç©æ‰‹æœº',
            'resting': 'ä¼‘æ¯',
            'unknown': 'æœªçŸ¥'
        }
        
        # ç»˜åˆ¶å§¿æ€è¾¹ç•Œæ¡†
        if "bbox" in behavior:
            bbox = behavior["bbox"]
            color = self.behavior_colors.get(behavior["head_pose"], (255, 255, 255))
            
            cv2.rectangle(frame,
                        (bbox["x1"], bbox["y1"]),
                        (bbox["x2"], bbox["y2"]),
                        color, 3)  # åŠ ç²—è¾¹æ¡†
            
            # ä½¿ç”¨PILç»˜åˆ¶ä¸­æ–‡
            pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(pil_img)
            
            # åŠ è½½ä¸­æ–‡å­—ä½“
            try:
                font = ImageFont.truetype("/System/Library/Fonts/PingFang.ttc", 20)
            except:
                try:
                    font = ImageFont.truetype("/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc", 20)
                except:
                    font = ImageFont.load_default()
            
            # ç»˜åˆ¶å­¦ç”Ÿå§“å
            name_position = (bbox["x1"], max(0, bbox["y1"] - 60))
            name_text = f"ğŸ‘¤ {student_name}"
            bbox_name = draw.textbbox(name_position, name_text, font=font)
            draw.rectangle(bbox_name, fill=(255, 100, 0, 200))
            draw.text(name_position, name_text, fill=(255, 255, 255), font=font)
            
            # ç»˜åˆ¶è¡Œä¸ºæ ‡ç­¾
            head_pose_label = behavior_labels.get(behavior["head_pose"], behavior["head_pose"])
            hand_activity_label = behavior_labels.get(behavior["hand_activity"], behavior["hand_activity"])
            label = f'{head_pose_label} / {hand_activity_label}'
            
            text_position = (bbox["x1"], max(0, bbox["y1"] - 30))
            bbox_text = draw.textbbox(text_position, label, font=font)
            draw.rectangle(bbox_text, fill=(0, 0, 0, 180))
            draw.text(text_position, label, fill=color, font=font)
            
            # è½¬å› OpenCV æ ¼å¼
            frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
        
        return frame
    
    def _summarize_individual_analysis(
        self, 
        frame_results: List[Dict],
        student_name: str
    ) -> Dict:
        """æ±‡æ€»ä¸ªäººè¡Œä¸ºåˆ†æç»“æœ"""
        if not frame_results:
            return {
                "error": f"åœ¨æ‰€æœ‰å¸§ä¸­éƒ½æœªæ‰¾åˆ°å­¦ç”Ÿ {student_name}",
                "behavior_percentages": {
                    "looking_up": 0.0,
                    "looking_down": 0.0,
                    "neutral": 0.0,
                    "writing": 0.0,
                    "using_phone": 0.0,
                    "resting": 0.0,
                    "unknown": 0.0
                },
                "recognition_score": 0,
                "total_frames_analyzed": 0
            }
        
        # ç»Ÿè®¡è¡Œä¸º
        head_pose_stats = {"looking_up": 0, "looking_down": 0, "neutral": 0}
        hand_activity_stats = {"writing": 0, "using_phone": 0, "resting": 0, "unknown": 0}
        
        total_frames = len(frame_results)
        total_similarity = 0
        
        for result in frame_results:
            if result.get("pose_found") and "behavior" in result:
                behavior = result["behavior"]
                
                # ç»Ÿè®¡å¤´éƒ¨å§¿æ€
                head_pose = behavior["head_pose"]
                if head_pose in head_pose_stats:
                    head_pose_stats[head_pose] += 1
                
                # ç»Ÿè®¡æ‰‹éƒ¨æ´»åŠ¨
                hand_activity = behavior["hand_activity"]
                if hand_activity in hand_activity_stats:
                    hand_activity_stats[hand_activity] += 1
                
                # ç´¯è®¡ç›¸ä¼¼åº¦
                total_similarity += behavior.get("face_similarity", 0)
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        behavior_percentages = {}
        for pose, count in head_pose_stats.items():
            behavior_percentages[pose] = round((count / total_frames) * 100, 2)
        for activity, count in hand_activity_stats.items():
            behavior_percentages[activity] = round((count / total_frames) * 100, 2)
        
        # è®¡ç®—è®¤çœŸç¨‹åº¦è¯„åˆ†
        looking_up_pct = behavior_percentages.get("looking_up", 0)
        writing_pct = behavior_percentages.get("writing", 0)
        using_phone_pct = behavior_percentages.get("using_phone", 0)
        
        attention_score = max(0, min(100, 
            looking_up_pct * 0.6 + writing_pct * 0.3 - using_phone_pct * 0.3
        ))
        
        # å¹³å‡è¯†åˆ«å‡†ç¡®åº¦
        avg_similarity = total_similarity / total_frames if total_frames > 0 else 0
        
        return {
            "behavior_stats": {**head_pose_stats, **hand_activity_stats},
            "behavior_percentages": behavior_percentages,
            "attention_score": round(attention_score, 2),
            "recognition_accuracy": round(avg_similarity * 100, 2),
            "total_frames_analyzed": total_frames,
            "conclusions": self._generate_individual_conclusions(behavior_percentages, attention_score)
        }
    
    def _generate_individual_conclusions(
        self, 
        behavior_percentages: Dict, 
        attention_score: float
    ) -> List[str]:
        """ç”Ÿæˆä¸ªäººåˆ†æç»“è®º"""
        conclusions = []
        
        looking_up = behavior_percentages.get("looking_up", 0)
        looking_down = behavior_percentages.get("looking_down", 0)
        writing = behavior_percentages.get("writing", 0)
        using_phone = behavior_percentages.get("using_phone", 0)
        
        # æ€»ä½“è¯„ä»·
        if attention_score >= 70:
            conclusions.append(f"æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼Œè®¤çœŸç¨‹åº¦è¯„åˆ† {attention_score:.0f}/100ï¼Œå­¦ä¹ æ€åº¦ç§¯æ")
        elif attention_score >= 50:
            conclusions.append(f"æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œè®¤çœŸç¨‹åº¦è¯„åˆ† {attention_score:.0f}/100ï¼Œæœ‰ä¸€å®šçš„å­¦ä¹ ä¸“æ³¨åº¦")
        else:
            conclusions.append(f"éœ€è¦æ”¹è¿›ï¼Œè®¤çœŸç¨‹åº¦è¯„åˆ† {attention_score:.0f}/100ï¼Œå»ºè®®æé«˜è¯¾å ‚å‚ä¸åº¦")
        
        # æŠ¬å¤´å¬è¯¾
        if looking_up > 60:
            conclusions.append(f"è¯¾å ‚æ³¨æ„åŠ›é›†ä¸­ï¼ŒæŠ¬å¤´å¬è¯¾å æ¯” {looking_up:.1f}%ï¼Œä¸“æ³¨åº¦é«˜")
        elif looking_up < 30:
            conclusions.append(f"æŠ¬å¤´å¬è¯¾æ—¶é—´è¾ƒå°‘ï¼ˆ{looking_up:.1f}%ï¼‰ï¼Œå»ºè®®æé«˜è¯¾å ‚ä¸“æ³¨åº¦")
        
        # è®°ç¬”è®°
        if writing > 30:
            conclusions.append(f"å­¦ä¹ ä¸»åŠ¨æ€§å¼ºï¼Œè®°ç¬”è®°æ—¶é—´å æ¯” {writing:.1f}%")
        
        # ä½¿ç”¨æ‰‹æœº
        if using_phone > 15:
            conclusions.append(f"ä½¿ç”¨æ‰‹æœºæ—¶é—´è¾ƒå¤šï¼ˆ{using_phone:.1f}%ï¼‰ï¼Œå»ºè®®å‡å°‘æ‰‹æœºä½¿ç”¨")
        
        return conclusions
